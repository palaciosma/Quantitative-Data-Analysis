---
title: "Lab4-part1"
author: "Isabel Sassoon"
date: "2022-10-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Lab 4 - Correlation and Linear Regression

This lab will cover the following topics:

 - Correlation
 - Linear regression
 - Interpreting the outputs and diagnostics
 - transforming data and applying lm

# Correlation

To demonstrate how to compute correlation in R we will make use of the cars data set that is available in base R.

```{r}
head(cars)
```
As you have seen in the lecture this has two variables: speed of the car, and the distance it took to come to a halt.

## How to compute correlation

```{r}
cor(cars$speed, cars$dist)
```

In order to visualize this data

```{r}
library(ggplot2)

ggplot(data=cars, aes(x=speed, y=dist)) + geom_point() + theme_classic() +
  ggtitle("Scatter Plot for speed vs distance")
```

It seems like this data is correlated. Lets test the correlation.
This will test an $H_0: \rho=0$ vs $H_1:\rho \ne 0$

```{r}
cor.test(cars$speed, cars$dist)
```

This results in a very small p-value, we reject the Null Hypothesis. These two variables are correlated.

# Linear Regression - cars

We may now want to model the relationship between the stopping distance and the speed. As the speed is what you can control it makes sense that it is the explanatory variable.

```{r}
cars.lm<-lm(cars$dist~cars$speed, data=cars)
cars.lm
```

This output gives us the minimum information: the intercept and the coefficient. From this we can see that:

distance $= -17.6+ 3.9\times$ speed.

To get more diagnostics we can use

```{r}
summary(cars.lm)
```

From the output above we learn that:
 - 0.65 of the variance is explained by the model
 - both coefficients (intercept and speed) are significant.
 
The relationship between the speed and the breaking distance is such that for every increase in speed of 1 - the breaking distance increases by 3.9324. 


## Diagnsotic plots

It is important to inspect the plots too:

```{r}
plot(cars.lm)
```
The residuals are acceptable but we can consider modeling this with the squared speed too.

```{r}
cars.lm2<-lm(cars$dist~cars$speed+ I(cars$speed^2))
```

How does our new model look?

```{r}
cars.lm2
```


```{r}
summary(cars.lm2)
```

and we should also look at the plots

```{r}
plot(cars.lm2)
```

The residuals for this model are fine, and don't point to any violations of the assumptions.
The $R^2$ value is slightly higher for this latter model that uses the squared speed, but it is a more complex model.


## What is the estimated stop distance for a new value of speed?

Either model can be used to get an estimate for a distance.

### What is the estimated breaking distance when the speed is 21?

If we use the first simpler model:

distance = -17.6+ 3.9 * speed

```{r}
dist.21<--17.6+ 3.9* 21
dist.21
```

For the more complex model
2.47014          0.91329          0.09996 

```{r}
dist2.21<-2.47+0.913*21+0.0996*21*21
dist2.21
```

The models give similar yet different estimates for the distance.

Both models are suitable. It is important that you consider what model you propose and why. You may want the model with higher $R^2$ or the less complex model...choice is yours! (But explain your rationale)


***

## Linear Regression - pulse rate
In another example for Linear Regression

A drug is developed to reduce pulse rate. The independent variable x is the dosage of the drug in mg; the dependent variable y is the reduction in pulse rate in beats per minute.

```{r}
x<-c(0.50,0.75 ,1.00,1.25,1.50,1.75,2.00,2.25,2.50,2.75,3.00,3.25,3.50)
y<-c(10,8,12,12,14,12,16,18,17,20,18,20,21)
pulse<-as.data.frame(cbind(x,y))

```

Plot the data

```{r}
ggplot(pulse, aes(x=x, y=y)) + geom_point() + ggtitle("Plot of x vs y")
```

Run a linear regression

```{r}
lm(pulse$y~pulse$x, data =pulse)
```

From this we can see that the relationship between x and y is:

$y=7.055+ 4.088 \times x$

### Running the diagnostic plots

```{r}
plot(lm(y~x))
```

### Summary output

This below is another more detailed output

```{r}
summary(lm(y~x))
```

This model has a very high $r^2$ and the difference between the variances is significant. So the model is very effective at explaining the variation in our Y values. Both coefficients are also significant.

### Predict - for a new x value compute the y predicted

If we have a value for x, we can use the regression equation to find an **estimate** for y.

$y=7.055+ 4.088 \times x$

Let find an estimate for y when x=1.15

```{r}
y.est<-7.055+4.088*1.15
y.est
```

### Transforming y?

We may want to transform y and see if we get a better model.
